{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558032e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfb1d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration (s): 23.08\n",
      "Total words: 30\n",
      "Words per minute: 77.99\n",
      "Unique words: 25\n",
      "Vocabulary richness: 0.83\n"
     ]
    }
   ],
   "source": [
    "audio_file_path = \"../samples/sample4.flac\"\n",
    "speech_context = \"conversational\"\n",
    "model = whisper.load_model(\"base\", device=\"cpu\")\n",
    "\n",
    "result = model.transcribe(\n",
    "    audio_file_path,\n",
    "    task=\"transcribe\",\n",
    "    word_timestamps=True,\n",
    "    fp16=False,\n",
    ")\n",
    "words = []\n",
    "for seg in result[\"segments\"]:\n",
    "    for w in seg[\"words\"]:\n",
    "        words.append({\n",
    "            \"word\": w[\"word\"].strip(),\n",
    "            \"start\": float(w[\"start\"]),\n",
    "            \"end\": float(w[\"end\"]),\n",
    "            \"duration\": float(w[\"end\"] - w[\"start\"]),\n",
    "            \"confidence\": float(w[\"probability\"])\n",
    "        })\n",
    "\n",
    "df_words = pd.DataFrame(words)\n",
    "df_words.sort_values(\"confidence\").head(5)\n",
    "\n",
    "segments = []\n",
    "for seg in result[\"segments\"]:\n",
    "    segments.append({\n",
    "        \"text\": seg[\"text\"].strip(),\n",
    "        \"start\": float(seg[\"start\"]),\n",
    "        \"end\": float(seg[\"end\"]),\n",
    "        \"duration\": float(float(seg[\"end\"]) - float(seg[\"start\"])),\n",
    "        \"avg_word_confidence\": sum([float(w[\"probability\"]) for w in seg[\"words\"]]) / (len(seg[\"words\"]) if len(seg[\"words\"]) > 0 else 0.0)\n",
    "    })\n",
    "\n",
    "df_segments = pd.DataFrame(segments)\n",
    "\n",
    "total_duration = float(df_segments.iloc[-1]['end']) #- df_segments.iloc[0]['start'])\n",
    "words_per_minute = (len(df_words) * 60) / (total_duration) \n",
    "\n",
    "\n",
    "\n",
    "pauses = df_words[\"start\"].iloc[1:].values - df_words[\"end\"].iloc[:-1].values\n",
    "long_pauses = pauses[pauses > 1.0]\n",
    "very_long_pauses = pauses[pauses > 2.0]\n",
    "\n",
    "words_clean = df_words['word'].str.lower()\n",
    "words_unique = words_clean.nunique()\n",
    "words_total = len(words_clean)\n",
    "vocab_richness = words_unique / words_total if words_total > 0 else 0\n",
    "top_repeats = words_clean.value_counts().head(5)\n",
    "print(f\"Total duration (s): {total_duration:.2f}\")\n",
    "print(f\"Total words: {words_total}\")\n",
    "print(f\"Words per minute: {words_per_minute:.2f}\")\n",
    "print(f\"Unique words: {words_unique}\")\n",
    "print(f\"Vocabulary richness: {vocab_richness:.2f}\")\n",
    "\n",
    "df_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "c6f866e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\") #Audio → numbers → shape the model expects\n",
    "wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\") #The model itself\n",
    "wav2vec.eval(); #Set to eval mode (we are not training)\n",
    "\n",
    "waveform, sr = sf.read(audio_file_path, dtype=\"float32\") # waveform: np.ndarray, sr: int, each number in waveform signifies amplitude at that time\n",
    "\n",
    "# Convert stereo → mono\n",
    "if waveform.ndim == 2:\n",
    "    waveform = waveform.mean(axis=1)\n",
    "\n",
    "waveform = torch.from_numpy(waveform) # Convert to torch tensor\n",
    "\n",
    "# Resample to 16kHz if needed (since the model expects 16kHz audio)\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "    sr = 16000\n",
    "\n",
    "# Prepare input for the model\n",
    "inputs = processor(\n",
    "    waveform.squeeze(), # Remove any extra dimensions\n",
    "    sampling_rate=16000,\n",
    "    return_tensors=\"pt\", # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    logits = wav2vec(**inputs).logits # logit = time_step_1 -> [prob_sound_1, ...]\n",
    "\n",
    "# for each time slice, choose sound with highest probability\n",
    "predicted_ids = torch.argmax(logits, dim=-1)[0]\n",
    "# Convert token IDs to human readable tokens\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(predicted_ids.tolist()) #tokens # each token is 20ms of audio that represents what was spoken\n",
    "# <pad> = nothing meaningful happened in this 20ms slice\n",
    "# | = word boundary\n",
    "\n",
    "FRAME_SEC = 0.02  # 20 ms per token\n",
    "\n",
    "events = []\n",
    "current = None\n",
    "\n",
    "for i, tok in enumerate(tokens):\n",
    "    t = i * FRAME_SEC\n",
    "\n",
    "    if tok == '<pad>':\n",
    "        if current:\n",
    "            current[\"end\"] = t\n",
    "            events.append(current)\n",
    "            current = None\n",
    "        continue\n",
    "\n",
    "    if current and current[\"label\"] == tok:\n",
    "        # same sound continuing (elongation)\n",
    "        continue\n",
    "\n",
    "    if current:\n",
    "        current[\"end\"] = t\n",
    "        events.append(current)\n",
    "\n",
    "    current = {\n",
    "        \"label\": tok,\n",
    "        \"start\": t\n",
    "    }\n",
    "\n",
    "if current:\n",
    "    current[\"end\"] = t\n",
    "    events.append(current)\n",
    "\n",
    "df_wav2vec = pd.DataFrame(events)\n",
    "\n",
    "df_wav2vec[\"duration\"] = df_wav2vec[\"end\"] - df_wav2vec[\"start\"]\n",
    "df_wav2vec[\"labels\"] = df_wav2vec[\"label\"]\n",
    "\n",
    "\n",
    "# collapse many tiny events of the same label into one\n",
    "merged = []\n",
    "current = None\n",
    "\n",
    "for _, row in df_wav2vec.iterrows():\n",
    "    if current is None:\n",
    "        current = row.to_dict()\n",
    "        continue\n",
    "\n",
    "    if row[\"label\"] == current[\"label\"] and row[\"start\"] <= current[\"end\"] + 0.04:\n",
    "        # same sound, extend it\n",
    "        current[\"end\"] = row[\"end\"]\n",
    "    else:\n",
    "        merged.append(current)\n",
    "        current = row.to_dict()\n",
    "\n",
    "if current:\n",
    "    merged.append(current)\n",
    "\n",
    "df_wav2vec_merged = pd.DataFrame(merged)\n",
    "df_wav2vec_merged[\"duration\"] = (\n",
    "    df_wav2vec_merged[\"end\"] - df_wav2vec_merged[\"start\"]\n",
    ")\n",
    "\n",
    "pauses = []\n",
    "\n",
    "for i in range(1, len(events)):\n",
    "    gap = events[i][\"start\"] - events[i-1][\"end\"]\n",
    "    if gap >= 0.3:  # 300 ms\n",
    "        pauses.append({\n",
    "            \"type\": \"pause\",\n",
    "            \"start\": events[i-1][\"end\"],\n",
    "            \"end\": events[i][\"start\"],\n",
    "            \"duration\": gap\n",
    "        })\n",
    "\n",
    "pauses;\n",
    "\n",
    "\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "def load_audio(path):\n",
    "    audio, sr = sf.read(path, dtype=\"float32\")\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)  # stereo → mono\n",
    "    if sr != 16000:\n",
    "        audio = torchaudio.functional.resample(\n",
    "            torch.from_numpy(audio), sr, 16000\n",
    "        ).numpy()\n",
    "        sr = 16000\n",
    "    return audio, sr\n",
    "\n",
    "audio, sr = load_audio(audio_file_path)\n",
    "\n",
    "import whisperx\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "align_model, metadata = whisperx.load_align_model(\n",
    "    language_code=\"en\",  # or result[\"language\"] if available\n",
    "    device=device\n",
    ")\n",
    "\n",
    "aligned = whisperx.align(\n",
    "    result[\"segments\"],  # <-- your existing Whisper output\n",
    "    align_model,\n",
    "    metadata,\n",
    "    audio,\n",
    "    device\n",
    ")\n",
    "\n",
    "aligned_words = []\n",
    "\n",
    "for seg in aligned[\"segments\"]:\n",
    "    for w in seg.get(\"words\", []):\n",
    "        if w[\"start\"] is not None and w[\"end\"] is not None:\n",
    "            aligned_words.append({\n",
    "                \"word\": w[\"word\"].strip().lower(),\n",
    "                \"start\": float(w[\"start\"]),\n",
    "                \"end\": float(w[\"end\"])\n",
    "            })\n",
    "\n",
    "df_aligned_words = pd.DataFrame(aligned_words)\n",
    "df_aligned_words = df_aligned_words.sort_values(\"start\").reset_index(drop=True)\n",
    "# df_aligned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "896c512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>raw_label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>Um,</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.029309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>M</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.822253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>8.08</td>\n",
       "      <td>8.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.475992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>11.04</td>\n",
       "      <td>11.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>16.02</td>\n",
       "      <td>16.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.96</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.748958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>21.70</td>\n",
       "      <td>21.72</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>21.82</td>\n",
       "      <td>21.84</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>21.96</td>\n",
       "      <td>21.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>22.06</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>N</td>\n",
       "      <td>23.72</td>\n",
       "      <td>23.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>24.04</td>\n",
       "      <td>24.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>24.18</td>\n",
       "      <td>24.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>24.68</td>\n",
       "      <td>24.70</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>24.98</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>25.44</td>\n",
       "      <td>25.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>25.58</td>\n",
       "      <td>25.60</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>25.80</td>\n",
       "      <td>25.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.36</td>\n",
       "      <td>26.38</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.78</td>\n",
       "      <td>26.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.94</td>\n",
       "      <td>26.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>27.18</td>\n",
       "      <td>27.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     style     type text raw_label  start    end  duration  confidence\n",
       "0    clear   filler   um       Um,   0.00   1.12      1.12    0.029309\n",
       "1   subtle   filler   uh         A   1.46   1.48      0.02         NaN\n",
       "2   subtle   filler   um         M   1.78   1.80      0.02         NaN\n",
       "3    clear   filler   um       um,   1.90   1.90      0.00    0.822253\n",
       "4   subtle   filler   uh         O   3.22   3.24      0.02         NaN\n",
       "5    clear   filler   um       um,   8.08   8.98      0.90    0.475992\n",
       "6   subtle  stutter    s         S  11.04  11.06      0.02         NaN\n",
       "7   subtle  stutter    s         S  11.26  11.28      0.02         NaN\n",
       "8    clear   filler   um       um,  16.02  16.24      0.22    0.695200\n",
       "9    clear   filler   um       um,  17.34  17.96      0.62    0.748958\n",
       "10  subtle  stutter    t         T  21.70  21.72      0.02         NaN\n",
       "11  subtle  stutter    t         T  21.82  21.84      0.02         NaN\n",
       "12  subtle  stutter    t         T  21.96  21.98      0.02         NaN\n",
       "13  subtle  stutter    t         T  22.06  22.08      0.02         NaN\n",
       "14  subtle   filler   um         N  23.72  23.76      0.04         NaN\n",
       "15  subtle  stutter    t         T  24.04  24.06      0.02         NaN\n",
       "16  subtle  stutter    t         T  24.18  24.20      0.02         NaN\n",
       "17  subtle  stutter    t         T  24.68  24.70      0.02         NaN\n",
       "18  subtle  stutter    t         T  24.98  25.00      0.02         NaN\n",
       "19  subtle  stutter    t         T  25.44  25.46      0.02         NaN\n",
       "20  subtle  stutter    t         T  25.58  25.60      0.02         NaN\n",
       "21  subtle  stutter    s         S  25.80  25.82      0.02         NaN\n",
       "22  subtle  stutter    t         T  26.20  26.22      0.02         NaN\n",
       "23  subtle  stutter    t         T  26.36  26.38      0.02         NaN\n",
       "24  subtle  stutter    t         T  26.78  26.80      0.02         NaN\n",
       "25  subtle  stutter    t         T  26.94  26.96      0.02         NaN\n",
       "26  subtle  stutter    s         S  27.18  27.20      0.02         NaN"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import whisper\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "\n",
    "FILLER_MAP = {\n",
    "    \"A\": \"uh\",\n",
    "    \"E\": \"uh\",\n",
    "    \"U\": \"uh\",\n",
    "    \"M\": \"um\",\n",
    "    \"N\": \"um\",\n",
    "    \"MM\": \"um\",\n",
    "    \"NN\": \"um\",\n",
    "}\n",
    "\n",
    "WORD_ONSET_WINDOW = 0.12       # window BEFORE word start\n",
    "MIN_FILLER_DURATION = 0.02\n",
    "STUTTER_CONSONANTS = set(\"BCDFGHJKLPQRSTVWXYZ\")\n",
    "\n",
    "# ==============================\n",
    "# HELPERS\n",
    "# ==============================\n",
    "\n",
    "def overlaps_any_word_relaxed(start, end, words, tol_before=0.02, tol_after=0.02):\n",
    "    for _, w in words.iterrows():\n",
    "        if start < w[\"end\"] + tol_after and end > w[\"start\"] - tol_before:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_word_initial_candidate(row, word_starts, max_lead=WORD_ONSET_WINDOW):\n",
    "    end = row[\"end\"]\n",
    "    upcoming = word_starts[\n",
    "        (word_starts[\"start\"] >= end) &\n",
    "        ((word_starts[\"start\"] - end) <= max_lead)\n",
    "    ]\n",
    "    return not upcoming.empty\n",
    "\n",
    "\n",
    "def merge_adjacent_events(df, max_gap=0.05):\n",
    "    merged = []\n",
    "    current = None\n",
    "\n",
    "    for _, row in df.sort_values(\"start\").iterrows():\n",
    "        if current is None:\n",
    "            current = row.to_dict()\n",
    "            continue\n",
    "\n",
    "        same_label = row[\"labels\"] == current[\"labels\"]\n",
    "        close = row[\"start\"] - current[\"end\"] <= max_gap\n",
    "\n",
    "        if same_label and close:\n",
    "            current[\"end\"] = row[\"end\"]\n",
    "            current[\"duration\"] += row[\"duration\"]\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            current = row.to_dict()\n",
    "\n",
    "    if current:\n",
    "        merged.append(current)\n",
    "\n",
    "    return pd.DataFrame(merged)\n",
    "\n",
    "\n",
    "def looks_like_filler(norm, duration):\n",
    "    if duration < MIN_FILLER_DURATION:\n",
    "        return False\n",
    "\n",
    "    # vowel hesitations (uh, ah, eh, h)\n",
    "    if re.fullmatch(r\"[AEIOUH]+\", norm):\n",
    "        return True\n",
    "\n",
    "    # nasal hums (mm, nn)\n",
    "    if re.fullmatch(r\"M+|N+\", norm):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def should_suppress_word_initial(row):\n",
    "    label = row[\"labels\"].upper()\n",
    "    norm = re.sub(r\"(.)\\1+\", r\"\\1\", label)\n",
    "\n",
    "    # NEVER suppress filler-shaped sounds\n",
    "    if looks_like_filler(norm, row[\"duration\"]):\n",
    "        return False\n",
    "\n",
    "    # suppress only ultra-short junk\n",
    "    return row[\"duration\"] < 0.03\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: REMOVE WORD-OVERLAPS\n",
    "# ==============================\n",
    "\n",
    "df_wav2vec_merged = df_wav2vec_merged.copy()\n",
    "\n",
    "df_wav2vec_merged[\"overlaps_word\"] = df_wav2vec_merged.apply(\n",
    "    lambda r: overlaps_any_word_relaxed(\n",
    "        r[\"start\"], r[\"end\"], df_aligned_words\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word = df_wav2vec_merged.loc[\n",
    "    ~df_wav2vec_merged[\"overlaps_word\"]\n",
    "].copy()\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: WORD-START HANDLING (FIXED)\n",
    "# ==============================\n",
    "\n",
    "word_starts = df_aligned_words[[\"start\"]].copy()\n",
    "\n",
    "df_non_word[\"is_word_initial\"] = df_non_word.apply(\n",
    "    lambda r: is_word_initial_candidate(r, word_starts),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word[\"suppress\"] = df_non_word.apply(\n",
    "    lambda r: r[\"is_word_initial\"] and should_suppress_word_initial(r),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word = df_non_word.loc[\n",
    "    ~df_non_word[\"suppress\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: MERGE MICRO EVENTS\n",
    "# ==============================\n",
    "\n",
    "df_non_word = merge_adjacent_events(df_non_word, max_gap=0.05)\n",
    "\n",
    "# ==============================\n",
    "# STEP 4: CLASSIFY WAV2VEC EVENTS\n",
    "# ==============================\n",
    "\n",
    "def classify_non_word_event(row):\n",
    "    label = row[\"labels\"].upper()\n",
    "    duration = row[\"duration\"]\n",
    "    norm = re.sub(r\"(.)\\1+\", r\"\\1\", label)\n",
    "\n",
    "    # ---- FILLERS ----\n",
    "    if looks_like_filler(norm, duration):\n",
    "        return {\n",
    "            \"type\": \"filler\",\n",
    "            \"raw_label\": label,\n",
    "            \"text\": FILLER_MAP.get(norm, \"uh\"),\n",
    "        }\n",
    "\n",
    "    # ---- STUTTERS ----\n",
    "    if (\n",
    "        norm in STUTTER_CONSONANTS\n",
    "        and norm not in FILLER_MAP\n",
    "        and duration < 0.15\n",
    "    ):\n",
    "        return {\n",
    "            \"type\": \"stutter\",\n",
    "            \"raw_label\": label,\n",
    "            \"text\": norm.lower(),\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "converted = []\n",
    "\n",
    "for _, row in df_non_word.iterrows():\n",
    "    result = classify_non_word_event(row)\n",
    "    if result:\n",
    "        converted.append({\n",
    "            \"type\": result[\"type\"],\n",
    "            \"text\": result[\"text\"],\n",
    "            \"raw_label\": result[\"raw_label\"],\n",
    "            \"start\": row[\"start\"],\n",
    "            \"end\": row[\"end\"],\n",
    "            \"duration\": row[\"duration\"],\n",
    "        })\n",
    "\n",
    "df_filler_events = pd.DataFrame(converted)\n",
    "\n",
    "# ==============================\n",
    "# STEP 5: WHISPER VERBATIM FILLERS\n",
    "# ==============================\n",
    "\n",
    "filler_model = whisper.load_model(\"base\", device=\"cpu\")\n",
    "\n",
    "verbatim_result = filler_model.transcribe(\n",
    "    audio_file_path,\n",
    "    task=\"transcribe\",\n",
    "    temperature=0,\n",
    "    word_timestamps=True,\n",
    "    condition_on_previous_text=False,\n",
    "    initial_prompt=(\n",
    "        \"Transcribe verbatim. Include filler words like um, uh, er, \"\n",
    "        \"false starts, repetitions, and hesitations.\"\n",
    "    ),\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "FILLER_PATTERN = re.compile(\n",
    "    r\"^(um+|uh+|erm+|er+|ah+|eh+)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def normalize_whisper_token(token):\n",
    "    token = token.lower().strip()\n",
    "    return re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", token)\n",
    "\n",
    "\n",
    "whisper_fillers = []\n",
    "\n",
    "for seg in verbatim_result.get(\"segments\", []):\n",
    "    for w in seg.get(\"words\", []):\n",
    "        norm = normalize_whisper_token(w[\"word\"])\n",
    "        if norm and FILLER_PATTERN.match(norm):\n",
    "            whisper_fillers.append({\n",
    "                \"style\": \"clear\",\n",
    "                \"type\": \"filler\",\n",
    "                \"text\": norm,\n",
    "                \"raw_label\": w[\"word\"],\n",
    "                \"start\": float(w[\"start\"]),\n",
    "                \"end\": float(w[\"end\"]),\n",
    "                \"duration\": float(w[\"end\"] - w[\"start\"]),\n",
    "                \"confidence\": float(w[\"probability\"]),\n",
    "            })\n",
    "\n",
    "df_whisper_fillers = pd.DataFrame(whisper_fillers)\n",
    "\n",
    "# ==============================\n",
    "# STEP 6: MERGE (WHISPER FIRST, WAV2VEC BACKFILL)\n",
    "# ==============================\n",
    "\n",
    "def overlaps_time(a_start, a_end, b_start, b_end, tol=0.05):\n",
    "    return (a_start < b_end + tol) and (a_end > b_start - tol)\n",
    "\n",
    "\n",
    "final_fillers = []\n",
    "\n",
    "for _, wf in df_whisper_fillers.iterrows():\n",
    "    final_fillers.append(wf.to_dict())\n",
    "\n",
    "for _, row in df_filler_events.iterrows():\n",
    "    duplicate = False\n",
    "\n",
    "    for af in final_fillers:\n",
    "        if overlaps_time(\n",
    "            row[\"start\"], row[\"end\"],\n",
    "            af[\"start\"], af[\"end\"]\n",
    "        ):\n",
    "            duplicate = True\n",
    "            break\n",
    "\n",
    "    if not duplicate:\n",
    "        final_fillers.append({\n",
    "            \"style\": \"subtle\",\n",
    "            \"type\": row[\"type\"],\n",
    "            \"text\": row[\"text\"],\n",
    "            \"raw_label\": row[\"raw_label\"],\n",
    "            \"start\": row[\"start\"],\n",
    "            \"end\": row[\"end\"],\n",
    "            \"duration\": row[\"duration\"],\n",
    "        })\n",
    "\n",
    "df_final_fillers = pd.DataFrame(final_fillers)\n",
    "\n",
    "if not df_final_fillers.empty:\n",
    "    df_final_fillers = (\n",
    "        df_final_fillers\n",
    "        .sort_values(\"start\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "df_final_fillers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c5167103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>raw_label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>confidence</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>Um,</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.029309</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>M</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.822253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>8.08</td>\n",
       "      <td>8.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.475992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>16.02</td>\n",
       "      <td>16.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>17.34</td>\n",
       "      <td>17.96</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.748958</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>N</td>\n",
       "      <td>23.72</td>\n",
       "      <td>23.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>24.04</td>\n",
       "      <td>24.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>25.44</td>\n",
       "      <td>25.60</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.38</td>\n",
       "      <td>0.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>26.78</td>\n",
       "      <td>26.96</td>\n",
       "      <td>0.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     style     type text raw_label  start    end  duration  confidence  count\n",
       "0    clear   filler   um       Um,   0.00   1.12      1.12    0.029309    NaN\n",
       "1   subtle   filler   uh         A   1.46   1.48      0.02         NaN    NaN\n",
       "2   subtle   filler   um         M   1.78   1.80      0.02         NaN    NaN\n",
       "3    clear   filler   um       um,   1.90   1.90      0.00    0.822253    NaN\n",
       "4   subtle   filler   uh         O   3.22   3.24      0.02         NaN    NaN\n",
       "5    clear   filler   um       um,   8.08   8.98      0.90    0.475992    NaN\n",
       "6    clear   filler   um       um,  16.02  16.24      0.22    0.695200    NaN\n",
       "7    clear   filler   um       um,  17.34  17.96      0.62    0.748958    NaN\n",
       "8   subtle  stutter    t         T  21.70  22.08      0.38         NaN    4.0\n",
       "9   subtle   filler   um         N  23.72  23.76      0.04         NaN    NaN\n",
       "10  subtle  stutter    t         T  24.04  24.20      0.16         NaN    2.0\n",
       "11  subtle  stutter    t         T  25.44  25.60      0.16         NaN    2.0\n",
       "12  subtle  stutter    t         T  26.20  26.38      0.18         NaN    2.0\n",
       "13  subtle  stutter    t         T  26.78  26.96      0.18         NaN    2.0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stutters = df_final_fillers[\n",
    "    df_final_fillers[\"type\"] == \"stutter\"\n",
    "].copy()\n",
    "\n",
    "df_fillers = df_final_fillers[\n",
    "    df_final_fillers[\"type\"] == \"filler\"\n",
    "].copy()\n",
    "\n",
    "GROUP_GAP_SEC = 0.15  # max gap between repetitions\n",
    "\n",
    "# ==============================\n",
    "# GROUP STUTTERS\n",
    "# ==============================\n",
    "\n",
    "grouped = []\n",
    "current = None\n",
    "\n",
    "for _, row in df_stutters.sort_values(\"start\").iterrows():\n",
    "    if current is None:\n",
    "        current = row.to_dict()\n",
    "        current[\"count\"] = 1\n",
    "        continue\n",
    "\n",
    "    same_sound = row[\"raw_label\"] == current[\"raw_label\"]\n",
    "    close_in_time = row[\"start\"] - current[\"end\"] <= GROUP_GAP_SEC\n",
    "\n",
    "    if same_sound and close_in_time:\n",
    "        current[\"end\"] = row[\"end\"]\n",
    "        current[\"duration\"] = current[\"end\"] - current[\"start\"]\n",
    "        current[\"count\"] += 1\n",
    "    else:\n",
    "        grouped.append(current)\n",
    "        current = row.to_dict()\n",
    "        current[\"count\"] = 1\n",
    "\n",
    "if current:\n",
    "    grouped.append(current)\n",
    "\n",
    "df_grouped_stutters = pd.DataFrame(grouped)\n",
    "\n",
    "\n",
    "df_final_clean = ( \n",
    "    pd.concat([df_fillers, df_grouped_stutters], ignore_index=True) \n",
    "    .sort_values(\"start\") \n",
    "    .reset_index(drop=True) \n",
    ")\n",
    "\n",
    "# --- HARD STUTTER FILTER --- \n",
    "# if it does not repeat and is very short, remove it\n",
    "df_final_clean = df_final_clean[\n",
    "    ~(\n",
    "        (df_final_clean[\"type\"] == \"stutter\") & \n",
    "        (df_final_clean[\"count\"].fillna(1) < 2) & \n",
    "        (df_final_clean[\"duration\"] < 0.15) \n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_final_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "50cce14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wpm': 77.98960138648181,\n",
       " 'fillers_per_min': np.float64(9.200000000000001),\n",
       " 'stutters_per_min': 10.0,\n",
       " 'long_pauses_per_min': 0.0,\n",
       " 'very_long_pauses_per_min': 0.0,\n",
       " 'pause_time_ratio': np.float64(0.05199306759098784),\n",
       " 'pause_variability': 0.0,\n",
       " 'vocab_richness': 0.8333333333333334,\n",
       " 'repetition_ratio': np.float64(0.06666666666666667)}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def filler_weight(duration):\n",
    "    \"\"\"\n",
    "    Weight fillers by perceptual impact.\n",
    "    \"\"\"\n",
    "    if duration < 0.08:\n",
    "        return 0.2      # micro hesitation\n",
    "    elif duration < 0.3:\n",
    "        return 0.6      # subtle filler\n",
    "    else:\n",
    "        return 1.0      # real filler\n",
    "\n",
    "\n",
    "def overlaps_filler(start, end, fillers, tol=0.05):\n",
    "    \"\"\"\n",
    "    Check whether a pause overlaps any filler or stutter event.\n",
    "    \"\"\"\n",
    "    for _, f in fillers.iterrows():\n",
    "        if start < f[\"end\"] + tol and end > f[\"start\"] - tol:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ENHANCED NORMALIZATION\n",
    "# ==============================\n",
    "\n",
    "duration_min = max(total_duration / 60.0, 0.5)\n",
    "\n",
    "# ---- Fillers & stutters (USE CLEANED EVENTS ONLY) ----\n",
    "filler_events = df_final_clean[df_final_clean[\"type\"] == \"filler\"]\n",
    "stutter_events = df_final_clean[df_final_clean[\"type\"] == \"stutter\"]\n",
    "\n",
    "fillers_per_min = (\n",
    "    filler_events[\"duration\"]\n",
    "    .apply(filler_weight)\n",
    "    .sum()\n",
    "    / duration_min\n",
    ")\n",
    "\n",
    "stutters_per_min = len(stutter_events) / duration_min\n",
    "\n",
    "\n",
    "# ---- Pause detection (durations only) ----\n",
    "pause_durations = []\n",
    "\n",
    "for i in range(1, len(df_words)):\n",
    "    gap_start = df_words.iloc[i - 1][\"end\"]\n",
    "    gap_end = df_words.iloc[i][\"start\"]\n",
    "    gap = gap_end - gap_start\n",
    "\n",
    "    if gap > 0.3 and not overlaps_filler(\n",
    "        gap_start, gap_end, df_final_clean\n",
    "    ):\n",
    "        pause_durations.append(gap)\n",
    "\n",
    "pause_durations = pd.Series(pause_durations, dtype=\"float\")\n",
    "\n",
    "\n",
    "# ---- Pause metrics ----\n",
    "long_pauses = pause_durations[pause_durations > 1.0]\n",
    "very_long_pauses = pause_durations[pause_durations > 2.0]\n",
    "\n",
    "long_pauses_per_min = len(long_pauses) / duration_min\n",
    "very_long_pauses_per_min = len(very_long_pauses) / duration_min\n",
    "\n",
    "pause_time_ratio = (\n",
    "    pause_durations.sum() / total_duration\n",
    "    if pause_durations.size > 0\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "pause_variability = (\n",
    "    pause_durations.std()\n",
    "    if pause_durations.size > 5\n",
    "    else 0.0\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Lexical metrics ----\n",
    "words_clean = df_words[\"word\"].str.lower()\n",
    "\n",
    "vocab_richness = words_clean.nunique() / len(words_clean)\n",
    "repetition_ratio = words_clean.value_counts().iloc[0] / len(words_clean)\n",
    "\n",
    "\n",
    "# ---- Final normalized metrics ----\n",
    "normalized_metrics = {\n",
    "    \"wpm\": words_per_minute,\n",
    "    \"fillers_per_min\": fillers_per_min,\n",
    "    \"stutters_per_min\": stutters_per_min,\n",
    "    \"long_pauses_per_min\": long_pauses_per_min,\n",
    "    \"very_long_pauses_per_min\": very_long_pauses_per_min,\n",
    "    \"pause_time_ratio\": pause_time_ratio,\n",
    "    \"pause_variability\": pause_variability,\n",
    "    \"vocab_richness\": vocab_richness,\n",
    "    \"repetition_ratio\": repetition_ratio,\n",
    "}\n",
    "\n",
    "normalized_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "eeee627c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': {'fluency_score': 62, 'readiness': 'borderline'},\n",
       " 'benchmarking': {'percentile': 30,\n",
       "  'target_score': 80,\n",
       "  'score_gap': 18,\n",
       "  'estimated_guided_practice_hours': 10.799999999999999},\n",
       " 'normalized_metrics': {'wpm': 77.98960138648181,\n",
       "  'fillers_per_min': np.float64(9.200000000000001),\n",
       "  'stutters_per_min': 10.0,\n",
       "  'long_pauses_per_min': 0.0,\n",
       "  'very_long_pauses_per_min': 0.0,\n",
       "  'pause_time_ratio': np.float64(0.05199306759098784),\n",
       "  'pause_variability': 0.0,\n",
       "  'vocab_richness': 0.8333333333333334,\n",
       "  'repetition_ratio': np.float64(0.06666666666666667)},\n",
       " 'opinions': {'primary_issues': [{'issue': 'filler_dependency',\n",
       "    'severity': 'high',\n",
       "    'root_cause': 'Fillers replace silent planning pauses.',\n",
       "    'score_impact': 25},\n",
       "   {'issue': 'delivery_pacing',\n",
       "    'severity': 'medium',\n",
       "    'root_cause': 'Speech rate deviates from optimal clarity range.',\n",
       "    'score_impact': 12}],\n",
       "  'action_plan': [{'priority': 1,\n",
       "    'focus': 'filler_dependency',\n",
       "    'instruction': 'Replace fillers with silent pauses under 300ms.',\n",
       "    'expected_score_gain': 12},\n",
       "   {'priority': 2,\n",
       "    'focus': 'delivery_pacing',\n",
       "    'instruction': 'Reduce speed slightly while maintaining energy.',\n",
       "    'expected_score_gain': 5}]},\n",
       " 'word_timestamps': [{'word': 'Writing',\n",
       "   'start': 1.88,\n",
       "   'end': 2.88,\n",
       "   'duration': 1.0,\n",
       "   'confidence': 0.31320732831954956},\n",
       "  {'word': 'Visual',\n",
       "   'start': 2.88,\n",
       "   'end': 3.6,\n",
       "   'duration': 0.7200000000000002,\n",
       "   'confidence': 0.44253888726234436},\n",
       "  {'word': 'Studio',\n",
       "   'start': 3.6,\n",
       "   'end': 4.02,\n",
       "   'duration': 0.4199999999999995,\n",
       "   'confidence': 0.9885483980178833},\n",
       "  {'word': 'Code',\n",
       "   'start': 4.02,\n",
       "   'end': 4.54,\n",
       "   'duration': 0.5200000000000005,\n",
       "   'confidence': 0.7354995012283325},\n",
       "  {'word': 'that',\n",
       "   'start': 4.54,\n",
       "   'end': 4.92,\n",
       "   'duration': 0.3799999999999999,\n",
       "   'confidence': 0.5884818434715271},\n",
       "  {'word': 'is',\n",
       "   'start': 4.92,\n",
       "   'end': 5.26,\n",
       "   'duration': 0.33999999999999986,\n",
       "   'confidence': 0.9888023734092712},\n",
       "  {'word': 'trying',\n",
       "   'start': 5.26,\n",
       "   'end': 6.2,\n",
       "   'duration': 0.9400000000000004,\n",
       "   'confidence': 0.9424266815185547},\n",
       "  {'word': 'to',\n",
       "   'start': 6.2,\n",
       "   'end': 6.4,\n",
       "   'duration': 0.20000000000000018,\n",
       "   'confidence': 0.9971023201942444},\n",
       "  {'word': 'solve',\n",
       "   'start': 6.4,\n",
       "   'end': 6.6,\n",
       "   'duration': 0.1999999999999993,\n",
       "   'confidence': 0.9995877146720886},\n",
       "  {'word': 'a',\n",
       "   'start': 6.6,\n",
       "   'end': 6.8,\n",
       "   'duration': 0.20000000000000018,\n",
       "   'confidence': 0.913078784942627},\n",
       "  {'word': 'problem',\n",
       "   'start': 6.8,\n",
       "   'end': 7.3,\n",
       "   'duration': 0.5,\n",
       "   'confidence': 0.99760901927948},\n",
       "  {'word': 'regarding',\n",
       "   'start': 7.3,\n",
       "   'end': 8.12,\n",
       "   'duration': 0.8199999999999994,\n",
       "   'confidence': 0.9842319488525391},\n",
       "  {'word': 'speech',\n",
       "   'start': 8.12,\n",
       "   'end': 10.28,\n",
       "   'duration': 2.16,\n",
       "   'confidence': 0.6356155276298523},\n",
       "  {'word': 'analysis.',\n",
       "   'start': 10.28,\n",
       "   'end': 10.98,\n",
       "   'duration': 0.7000000000000011,\n",
       "   'confidence': 0.9535420536994934},\n",
       "  {'word': 'I',\n",
       "   'start': 11.86,\n",
       "   'end': 12.46,\n",
       "   'duration': 0.6000000000000014,\n",
       "   'confidence': 0.9845004081726074},\n",
       "  {'word': 'want',\n",
       "   'start': 12.46,\n",
       "   'end': 12.64,\n",
       "   'duration': 0.17999999999999972,\n",
       "   'confidence': 0.9901144504547119},\n",
       "  {'word': 'to',\n",
       "   'start': 12.64,\n",
       "   'end': 12.84,\n",
       "   'duration': 0.1999999999999993,\n",
       "   'confidence': 0.986556351184845},\n",
       "  {'word': 'understand',\n",
       "   'start': 12.84,\n",
       "   'end': 13.94,\n",
       "   'duration': 1.0999999999999996,\n",
       "   'confidence': 0.9382566213607788},\n",
       "  {'word': 'how',\n",
       "   'start': 13.94,\n",
       "   'end': 15.08,\n",
       "   'duration': 1.1400000000000006,\n",
       "   'confidence': 0.969782292842865},\n",
       "  {'word': 'it',\n",
       "   'start': 15.08,\n",
       "   'end': 15.4,\n",
       "   'duration': 0.3200000000000003,\n",
       "   'confidence': 0.9899999499320984},\n",
       "  {'word': 'detects',\n",
       "   'start': 15.4,\n",
       "   'end': 16.0,\n",
       "   'duration': 0.5999999999999996,\n",
       "   'confidence': 0.9605817496776581},\n",
       "  {'word': 'and',\n",
       "   'start': 16.0,\n",
       "   'end': 16.66,\n",
       "   'duration': 0.6600000000000001,\n",
       "   'confidence': 0.24698400497436523},\n",
       "  {'word': 'yes,',\n",
       "   'start': 16.66,\n",
       "   'end': 20.6,\n",
       "   'duration': 3.9400000000000013,\n",
       "   'confidence': 0.4206799864768982},\n",
       "  {'word': 'yes,',\n",
       "   'start': 20.88,\n",
       "   'end': 21.0,\n",
       "   'duration': 0.120000000000001,\n",
       "   'confidence': 0.78538578748703},\n",
       "  {'word': 'and',\n",
       "   'start': 21.32,\n",
       "   'end': 21.32,\n",
       "   'duration': 0.0,\n",
       "   'confidence': 0.7108930349349976},\n",
       "  {'word': 'that',\n",
       "   'start': 21.32,\n",
       "   'end': 21.48,\n",
       "   'duration': 0.16000000000000014,\n",
       "   'confidence': 0.9878936409950256},\n",
       "  {'word': 'is',\n",
       "   'start': 21.48,\n",
       "   'end': 22.16,\n",
       "   'duration': 0.6799999999999997,\n",
       "   'confidence': 0.7237584590911865},\n",
       "  {'word': 'the',\n",
       "   'start': 22.16,\n",
       "   'end': 22.38,\n",
       "   'duration': 0.21999999999999886,\n",
       "   'confidence': 0.9781981706619263},\n",
       "  {'word': 'main',\n",
       "   'start': 22.38,\n",
       "   'end': 22.58,\n",
       "   'duration': 0.1999999999999993,\n",
       "   'confidence': 0.9858909845352173},\n",
       "  {'word': 'thing.',\n",
       "   'start': 22.58,\n",
       "   'end': 23.08,\n",
       "   'duration': 0.5,\n",
       "   'confidence': 0.9954002499580383}],\n",
       " 'segment_timestamps': [{'text': 'Writing Visual Studio Code that is trying to solve a problem regarding speech analysis.',\n",
       "   'start': 1.88,\n",
       "   'end': 10.98,\n",
       "   'duration': 9.100000000000001,\n",
       "   'avg_word_confidence': 0.8200194558926991},\n",
       "  {'text': 'I want to understand how it detects and yes, yes, and that is the main thing.',\n",
       "   'start': 11.86,\n",
       "   'end': 23.08,\n",
       "   'duration': 11.219999999999999,\n",
       "   'avg_word_confidence': 0.8534297589212656}],\n",
       " 'filler_events': [{'style': 'clear',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': ' Um,',\n",
       "   'start': 0.0,\n",
       "   'end': 1.12,\n",
       "   'duration': 1.12,\n",
       "   'confidence': 0.029308902099728584},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'filler',\n",
       "   'text': 'uh',\n",
       "   'raw_label': 'A',\n",
       "   'start': 1.46,\n",
       "   'end': 1.48,\n",
       "   'duration': 0.020000000000000018,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': 'M',\n",
       "   'start': 1.78,\n",
       "   'end': 1.8,\n",
       "   'duration': 0.020000000000000018,\n",
       "   'confidence': nan},\n",
       "  {'style': 'clear',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': ' um,',\n",
       "   'start': 1.9,\n",
       "   'end': 1.9,\n",
       "   'duration': 0.0,\n",
       "   'confidence': 0.8222531676292419},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'filler',\n",
       "   'text': 'uh',\n",
       "   'raw_label': 'O',\n",
       "   'start': 3.22,\n",
       "   'end': 3.24,\n",
       "   'duration': 0.020000000000000018,\n",
       "   'confidence': nan},\n",
       "  {'style': 'clear',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': ' um,',\n",
       "   'start': 8.08,\n",
       "   'end': 8.98,\n",
       "   'duration': 0.9000000000000004,\n",
       "   'confidence': 0.4759918451309204},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 's',\n",
       "   'raw_label': 'S',\n",
       "   'start': 11.040000000000001,\n",
       "   'end': 11.06,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 's',\n",
       "   'raw_label': 'S',\n",
       "   'start': 11.26,\n",
       "   'end': 11.28,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'clear',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': ' um,',\n",
       "   'start': 16.02,\n",
       "   'end': 16.24,\n",
       "   'duration': 0.21999999999999886,\n",
       "   'confidence': 0.6951995491981506},\n",
       "  {'style': 'clear',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': ' um,',\n",
       "   'start': 17.34,\n",
       "   'end': 17.96,\n",
       "   'duration': 0.620000000000001,\n",
       "   'confidence': 0.7489582896232605},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 21.7,\n",
       "   'end': 21.72,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 21.82,\n",
       "   'end': 21.84,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 21.96,\n",
       "   'end': 21.98,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 22.06,\n",
       "   'end': 22.080000000000002,\n",
       "   'duration': 0.020000000000003126,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'filler',\n",
       "   'text': 'um',\n",
       "   'raw_label': 'N',\n",
       "   'start': 23.72,\n",
       "   'end': 23.76,\n",
       "   'duration': 0.0400000000000027,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 24.04,\n",
       "   'end': 24.060000000000002,\n",
       "   'duration': 0.020000000000003126,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 24.18,\n",
       "   'end': 24.2,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 24.68,\n",
       "   'end': 24.7,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 24.98,\n",
       "   'end': 25.0,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 25.44,\n",
       "   'end': 25.46,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 25.580000000000002,\n",
       "   'end': 25.6,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 's',\n",
       "   'raw_label': 'S',\n",
       "   'start': 25.8,\n",
       "   'end': 25.82,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 26.2,\n",
       "   'end': 26.22,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 26.36,\n",
       "   'end': 26.38,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 26.78,\n",
       "   'end': 26.8,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 't',\n",
       "   'raw_label': 'T',\n",
       "   'start': 26.94,\n",
       "   'end': 26.96,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan},\n",
       "  {'style': 'subtle',\n",
       "   'type': 'stutter',\n",
       "   'text': 's',\n",
       "   'raw_label': 'S',\n",
       "   'start': 27.18,\n",
       "   'end': 27.2,\n",
       "   'duration': 0.019999999999999574,\n",
       "   'confidence': nan}],\n",
       " 'aligned_words': [{'word': 'writing', 'start': 1.88, 'end': 3.025},\n",
       "  {'word': 'visual', 'start': 3.387, 'end': 3.748},\n",
       "  {'word': 'studio', 'start': 3.768, 'end': 4.19},\n",
       "  {'word': 'code', 'start': 4.311, 'end': 4.632},\n",
       "  {'word': 'that', 'start': 4.692, 'end': 5.074},\n",
       "  {'word': 'is', 'start': 5.215, 'end': 5.315},\n",
       "  {'word': 'trying', 'start': 5.355, 'end': 6.32},\n",
       "  {'word': 'to', 'start': 6.36, 'end': 6.44},\n",
       "  {'word': 'solve', 'start': 6.48, 'end': 6.761},\n",
       "  {'word': 'a', 'start': 6.822, 'end': 6.862},\n",
       "  {'word': 'problem', 'start': 6.922, 'end': 7.565},\n",
       "  {'word': 'regarding', 'start': 7.706, 'end': 8.449},\n",
       "  {'word': 'speech', 'start': 10.116, 'end': 10.659},\n",
       "  {'word': 'analysis.', 'start': 10.679, 'end': 11.0},\n",
       "  {'word': 'i', 'start': 11.86, 'end': 12.482},\n",
       "  {'word': 'want', 'start': 12.542, 'end': 12.723},\n",
       "  {'word': 'to', 'start': 12.743, 'end': 12.904},\n",
       "  {'word': 'understand', 'start': 13.084, 'end': 14.108},\n",
       "  {'word': 'how', 'start': 14.148, 'end': 15.152},\n",
       "  {'word': 'it', 'start': 15.252, 'end': 15.352},\n",
       "  {'word': 'detects', 'start': 15.553, 'end': 15.935},\n",
       "  {'word': 'and', 'start': 16.175, 'end': 16.657},\n",
       "  {'word': 'yes,', 'start': 16.677, 'end': 18.082},\n",
       "  {'word': 'yes,', 'start': 20.511, 'end': 21.133},\n",
       "  {'word': 'and', 'start': 21.233, 'end': 21.354},\n",
       "  {'word': 'that', 'start': 21.414, 'end': 21.635},\n",
       "  {'word': 'is', 'start': 22.217, 'end': 22.277},\n",
       "  {'word': 'the', 'start': 22.317, 'end': 22.458},\n",
       "  {'word': 'main', 'start': 22.518, 'end': 22.759},\n",
       "  {'word': 'thing.', 'start': 22.839, 'end': 23.1}]}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FLUENCY ANALYSIS — SCORING, ISSUES, READINESS, BENCHMARKING\n",
    "# ============================================================\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ==============================\n",
    "\n",
    "MIN_ANALYSIS_DURATION_SEC = 5.0  \n",
    "\n",
    "# ---- Speech rate ----\n",
    "WPM_TOO_SLOW = 70\n",
    "WPM_SLOW_THRESHOLD = 110\n",
    "WPM_OPTIMAL_MAX = 170\n",
    "WPM_FAST_DECAY_RANGE = 120\n",
    "\n",
    "# ---- Pauses ----\n",
    "MAX_LONG_PAUSES_PER_MIN = 4.0\n",
    "PAUSE_SCORE_BLOCK_THRESHOLD = 0.6\n",
    "\n",
    "# ---- Fillers ----\n",
    "MAX_FILLERS_PER_MIN = 6.0\n",
    "FILLER_SCORE_BLOCK_THRESHOLD = 0.6\n",
    "\n",
    "# ---- Stability ----\n",
    "BASE_PAUSE_VARIABILITY = 0.7\n",
    "STABILITY_SCORE_WARN_THRESHOLD = 0.6\n",
    "\n",
    "# ---- Lexical ----\n",
    "LEXICAL_LOW_THRESHOLD = 0.5\n",
    "\n",
    "# ---- Aggregation weights (must sum to 1.0) ----\n",
    "WEIGHT_PAUSE = 0.30\n",
    "WEIGHT_FILLER = 0.25\n",
    "WEIGHT_STABILITY = 0.20\n",
    "WEIGHT_SPEECH_RATE = 0.15\n",
    "WEIGHT_LEXICAL = 0.10\n",
    "\n",
    "# ---- Readiness ----\n",
    "MIN_SAMPLE_DURATION_SEC = 30\n",
    "READY_SCORE_THRESHOLD = 80\n",
    "\n",
    "# ---- Benchmarking ----\n",
    "BENCHMARK_TARGET_SCORE = 80\n",
    "PRACTICE_HOURS_PER_POINT = 0.6\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# CONTEXT SENSITIVITY\n",
    "# ==============================\n",
    "\n",
    "CONTEXT_CONFIG = {\n",
    "    \"conversational\": {\n",
    "        \"pause_tolerance\": 1.0,\n",
    "        \"pause_variability_tolerance\": 1.0,\n",
    "    },\n",
    "    \"narrative\": {\n",
    "        \"pause_tolerance\": 1.4,   # storytelling allows reflection\n",
    "        \"pause_variability_tolerance\": 1.3,\n",
    "    },\n",
    "    \"presentation\": {\n",
    "        \"pause_tolerance\": 1.2,   # deliberate but structured\n",
    "        \"pause_variability_tolerance\": 1.1,\n",
    "    },\n",
    "    \"interview\": {\n",
    "        \"pause_tolerance\": 0.9,   # fast turn-taking expected\n",
    "        \"pause_variability_tolerance\": 0.9,\n",
    "    },\n",
    "}\n",
    "\n",
    "context = CONTEXT_CONFIG.get(\n",
    "    speech_context,\n",
    "    CONTEXT_CONFIG[\"conversational\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# HELPERS\n",
    "# ==============================\n",
    "\n",
    "def clamp01(x: float) -> float:\n",
    "    \"\"\"Clamp numeric values into [0, 1].\"\"\"\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# SUBSCORES (0–1, higher = better)\n",
    "# ==============================\n",
    "\n",
    "wpm = normalized_metrics[\"wpm\"]\n",
    "\n",
    "# ---- Speech rate (asymmetric penalty) ----\n",
    "if wpm < WPM_SLOW_THRESHOLD:\n",
    "    speech_rate_score = clamp01(\n",
    "        (wpm - WPM_TOO_SLOW) / (WPM_SLOW_THRESHOLD - WPM_TOO_SLOW)\n",
    "    )\n",
    "elif wpm <= WPM_OPTIMAL_MAX:\n",
    "    speech_rate_score = 1.0\n",
    "else:\n",
    "    speech_rate_score = clamp01(\n",
    "        1 - (wpm - WPM_OPTIMAL_MAX) / WPM_FAST_DECAY_RANGE\n",
    "    )\n",
    "\n",
    "\n",
    "# ---- Pause structure ----\n",
    "pause_score = clamp01(\n",
    "    1 - (\n",
    "        normalized_metrics[\"long_pauses_per_min\"]\n",
    "        / (MAX_LONG_PAUSES_PER_MIN * context[\"pause_tolerance\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Filler dependency ----\n",
    "filler_score = clamp01(\n",
    "    1 - (normalized_metrics[\"fillers_per_min\"] / MAX_FILLERS_PER_MIN)\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Rhythmic stability ----\n",
    "stability_score = clamp01(\n",
    "    1 - (\n",
    "        normalized_metrics[\"pause_variability\"]\n",
    "        / (BASE_PAUSE_VARIABILITY * context[\"pause_variability_tolerance\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Lexical quality (style only) ----\n",
    "lexical_score = clamp01(\n",
    "    0.65 * normalized_metrics[\"vocab_richness\"]\n",
    "    + 0.35 * (1 - normalized_metrics[\"repetition_ratio\"])\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# FLUENCY SCORE (0–100)\n",
    "# ==============================\n",
    "\n",
    "raw_score = (\n",
    "    WEIGHT_PAUSE * pause_score +\n",
    "    WEIGHT_FILLER * filler_score +\n",
    "    WEIGHT_STABILITY * stability_score +\n",
    "    WEIGHT_SPEECH_RATE * speech_rate_score +\n",
    "    WEIGHT_LEXICAL * lexical_score\n",
    ")\n",
    "\n",
    "fluency_score = int(round(100 * clamp01(raw_score)))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ISSUE DETECTION\n",
    "# ==============================\n",
    "\n",
    "issues = []\n",
    "\n",
    "def issue(severity, issue_id, root_cause, score_impact):\n",
    "    return {\n",
    "        \"issue\": issue_id,\n",
    "        \"severity\": severity,\n",
    "        \"root_cause\": root_cause,\n",
    "        \"score_impact\": score_impact,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Structural blockers ----\n",
    "if pause_score < PAUSE_SCORE_BLOCK_THRESHOLD:\n",
    "    issues.append(issue(\n",
    "        \"high\",\n",
    "        \"hesitation_structure\",\n",
    "        \"Pauses frequently interrupt sentence flow.\",\n",
    "        int((1 - pause_score) * 30),\n",
    "    ))\n",
    "\n",
    "if filler_score < FILLER_SCORE_BLOCK_THRESHOLD:\n",
    "    issues.append(issue(\n",
    "        \"high\",\n",
    "        \"filler_dependency\",\n",
    "        \"Fillers replace silent planning pauses.\",\n",
    "        int((1 - filler_score) * 25),\n",
    "    ))\n",
    "\n",
    "if stability_score < STABILITY_SCORE_WARN_THRESHOLD:\n",
    "    issues.append(issue(\n",
    "        \"medium\",\n",
    "        \"delivery_instability\",\n",
    "        \"Speech rhythm varies unpredictably.\",\n",
    "        int((1 - stability_score) * 20),\n",
    "    ))\n",
    "\n",
    "\n",
    "# ---- Style issues (never blockers alone) ----\n",
    "if speech_rate_score < 0.7:\n",
    "    issues.append(issue(\n",
    "        \"medium\",\n",
    "        \"delivery_pacing\",\n",
    "        \"Speech rate deviates from optimal clarity range.\",\n",
    "        int((1 - speech_rate_score) * 15),\n",
    "    ))\n",
    "\n",
    "if lexical_score < LEXICAL_LOW_THRESHOLD:\n",
    "    issues.append(issue(\n",
    "        \"low\",\n",
    "        \"lexical_simplicity\",\n",
    "        \"Frequent reuse of common vocabulary.\",\n",
    "        int((1 - lexical_score) * 10),\n",
    "    ))\n",
    "\n",
    "\n",
    "issues = sorted(issues, key=lambda x: x[\"score_impact\"], reverse=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# READINESS & SUFFICIENCY\n",
    "# ==============================\n",
    "\n",
    "if total_duration < MIN_ANALYSIS_DURATION_SEC:\n",
    "    readiness = \"insufficient_sample\"\n",
    "\n",
    "    verdict = {\n",
    "        \"fluency_score\": None,\n",
    "        \"readiness\": readiness,\n",
    "    }\n",
    "\n",
    "    benchmarking = None\n",
    "\n",
    "else:\n",
    "    # ---------- READINESS JUDGMENT ----------\n",
    "    high_issues = [i for i in issues if i[\"severity\"] == \"high\"]\n",
    "    medium_issues = [i for i in issues if i[\"severity\"] == \"medium\"]\n",
    "\n",
    "    if len(high_issues) >= 2:\n",
    "        readiness = \"not_ready\"\n",
    "    elif len(high_issues) == 1:\n",
    "        readiness = \"borderline\"\n",
    "    elif len(medium_issues) >= 2:\n",
    "        readiness = \"borderline\"\n",
    "    elif fluency_score >= 80:\n",
    "        readiness = \"ready\"\n",
    "    else:\n",
    "        readiness = \"borderline\"\n",
    "\n",
    "    verdict = {\n",
    "        \"fluency_score\": fluency_score,\n",
    "        \"readiness\": readiness,\n",
    "    }\n",
    "\n",
    "    # ---------- BENCHMARKING ----------\n",
    "    if fluency_score >= 85:\n",
    "        percentile = 80\n",
    "    elif fluency_score >= 75:\n",
    "        percentile = 65\n",
    "    elif fluency_score >= 65:\n",
    "        percentile = 50\n",
    "    else:\n",
    "        percentile = 30\n",
    "\n",
    "    score_gap = (\n",
    "        max(0, 80 - fluency_score)\n",
    "        if readiness != \"ready\"\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    benchmarking = {\n",
    "        \"percentile\": percentile,\n",
    "        \"target_score\": 80,\n",
    "        \"score_gap\": score_gap,\n",
    "        \"estimated_guided_practice_hours\": score_gap * 0.6,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ACTION PLAN (NORMALIZED GAINS)\n",
    "# ==============================\n",
    "\n",
    "action_plan = []\n",
    "\n",
    "max_gain = sum(i[\"score_impact\"] for i in issues[:3]) or 1\n",
    "scale = score_gap / max_gain if score_gap > 0 else 1.0\n",
    "\n",
    "INSTRUCTIONS = {\n",
    "    \"hesitation_structure\": \"Pause only after completing full clauses.\",\n",
    "    \"filler_dependency\": \"Replace fillers with silent pauses under 300ms.\",\n",
    "    \"delivery_instability\": \"Practice steady pacing with metronome drills.\",\n",
    "    \"delivery_pacing\": \"Reduce speed slightly while maintaining energy.\",\n",
    "    \"lexical_simplicity\": \"Actively substitute repeated words during rehearsal.\",\n",
    "}\n",
    "\n",
    "for idx, issue in enumerate(issues[:3]):\n",
    "    action_plan.append({\n",
    "        \"priority\": idx + 1,\n",
    "        \"focus\": issue[\"issue\"],\n",
    "        \"instruction\": INSTRUCTIONS[issue[\"issue\"]],\n",
    "        \"expected_score_gain\": int(issue[\"score_impact\"] * scale),\n",
    "    })\n",
    "\n",
    "opinions = {\n",
    "    \"primary_issues\": issues,\n",
    "    \"action_plan\": action_plan,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "final_response = {\n",
    "    \"verdict\": verdict,\n",
    "    \"benchmarking\": benchmarking,\n",
    "    \"normalized_metrics\": normalized_metrics,\n",
    "    \"opinions\": opinions,\n",
    "    \"word_timestamps\": df_words.to_dict(orient=\"records\"),\n",
    "    \"segment_timestamps\": df_segments.to_dict(orient=\"records\"),\n",
    "    \"filler_events\": df_final_fillers.to_dict(orient=\"records\"),\n",
    "    \"aligned_words\": df_aligned_words.to_dict(orient=\"records\"),\n",
    "}\n",
    "\n",
    "\n",
    "final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514459f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
