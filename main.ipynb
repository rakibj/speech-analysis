{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558032e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "adfb1d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration (s): 93.46\n",
      "Total words: 183\n",
      "Words per minute: 117.48\n",
      "Unique words: 98\n",
      "Vocabulary richness: 0.54\n"
     ]
    }
   ],
   "source": [
    "audio_file_path = \"sample5.flac\"\n",
    "speech_context = \"conversational\"\n",
    "model = whisper.load_model(\"base\", device=\"cpu\")\n",
    "\n",
    "result = model.transcribe(\n",
    "    audio_file_path,\n",
    "    task=\"transcribe\",\n",
    "    word_timestamps=True,\n",
    "    fp16=False,\n",
    ")\n",
    "words = []\n",
    "for seg in result[\"segments\"]:\n",
    "    for w in seg[\"words\"]:\n",
    "        words.append({\n",
    "            \"word\": w[\"word\"].strip(),\n",
    "            \"start\": float(w[\"start\"]),\n",
    "            \"end\": float(w[\"end\"]),\n",
    "            \"duration\": float(w[\"end\"] - w[\"start\"]),\n",
    "            \"confidence\": float(w[\"probability\"])\n",
    "        })\n",
    "\n",
    "df_words = pd.DataFrame(words)\n",
    "df_words.sort_values(\"confidence\").head(5)\n",
    "\n",
    "segments = []\n",
    "for seg in result[\"segments\"]:\n",
    "    segments.append({\n",
    "        \"text\": seg[\"text\"].strip(),\n",
    "        \"start\": float(seg[\"start\"]),\n",
    "        \"end\": float(seg[\"end\"]),\n",
    "        \"duration\": float(float(seg[\"end\"]) - float(seg[\"start\"])),\n",
    "        \"avg_word_confidence\": sum([float(w[\"probability\"]) for w in seg[\"words\"]]) / (len(seg[\"words\"]) if len(seg[\"words\"]) > 0 else 0.0)\n",
    "    })\n",
    "\n",
    "df_segments = pd.DataFrame(segments)\n",
    "\n",
    "total_duration = float(df_segments.iloc[-1]['end']) #- df_segments.iloc[0]['start'])\n",
    "words_per_minute = (len(df_words) * 60) / (total_duration) \n",
    "\n",
    "\n",
    "\n",
    "pauses = df_words[\"start\"].iloc[1:].values - df_words[\"end\"].iloc[:-1].values\n",
    "long_pauses = pauses[pauses > 1.0]\n",
    "very_long_pauses = pauses[pauses > 2.0]\n",
    "\n",
    "words_clean = df_words['word'].str.lower()\n",
    "words_unique = words_clean.nunique()\n",
    "words_total = len(words_clean)\n",
    "vocab_richness = words_unique / words_total if words_total > 0 else 0\n",
    "top_repeats = words_clean.value_counts().head(5)\n",
    "print(f\"Total duration (s): {total_duration:.2f}\")\n",
    "print(f\"Total words: {words_total}\")\n",
    "print(f\"Words per minute: {words_per_minute:.2f}\")\n",
    "print(f\"Unique words: {words_unique}\")\n",
    "print(f\"Vocabulary richness: {vocab_richness:.2f}\")\n",
    "\n",
    "df_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c6f866e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\") #Audio → numbers → shape the model expects\n",
    "wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\") #The model itself\n",
    "wav2vec.eval(); #Set to eval mode (we are not training)\n",
    "\n",
    "waveform, sr = sf.read(audio_file_path, dtype=\"float32\") # waveform: np.ndarray, sr: int, each number in waveform signifies amplitude at that time\n",
    "\n",
    "# Convert stereo → mono\n",
    "if waveform.ndim == 2:\n",
    "    waveform = waveform.mean(axis=1)\n",
    "\n",
    "waveform = torch.from_numpy(waveform) # Convert to torch tensor\n",
    "\n",
    "# Resample to 16kHz if needed (since the model expects 16kHz audio)\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "    sr = 16000\n",
    "\n",
    "# Prepare input for the model\n",
    "inputs = processor(\n",
    "    waveform.squeeze(), # Remove any extra dimensions\n",
    "    sampling_rate=16000,\n",
    "    return_tensors=\"pt\", # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    logits = wav2vec(**inputs).logits # logit = time_step_1 -> [prob_sound_1, ...]\n",
    "\n",
    "# for each time slice, choose sound with highest probability\n",
    "predicted_ids = torch.argmax(logits, dim=-1)[0]\n",
    "# Convert token IDs to human readable tokens\n",
    "tokens = processor.tokenizer.convert_ids_to_tokens(predicted_ids.tolist()) #tokens # each token is 20ms of audio that represents what was spoken\n",
    "# <pad> = nothing meaningful happened in this 20ms slice\n",
    "# | = word boundary\n",
    "\n",
    "FRAME_SEC = 0.02  # 20 ms per token\n",
    "\n",
    "events = []\n",
    "current = None\n",
    "\n",
    "for i, tok in enumerate(tokens):\n",
    "    t = i * FRAME_SEC\n",
    "\n",
    "    if tok == '<pad>':\n",
    "        if current:\n",
    "            current[\"end\"] = t\n",
    "            events.append(current)\n",
    "            current = None\n",
    "        continue\n",
    "\n",
    "    if current and current[\"label\"] == tok:\n",
    "        # same sound continuing (elongation)\n",
    "        continue\n",
    "\n",
    "    if current:\n",
    "        current[\"end\"] = t\n",
    "        events.append(current)\n",
    "\n",
    "    current = {\n",
    "        \"label\": tok,\n",
    "        \"start\": t\n",
    "    }\n",
    "\n",
    "if current:\n",
    "    current[\"end\"] = t\n",
    "    events.append(current)\n",
    "\n",
    "df_wav2vec = pd.DataFrame(events)\n",
    "\n",
    "df_wav2vec[\"duration\"] = df_wav2vec[\"end\"] - df_wav2vec[\"start\"]\n",
    "df_wav2vec[\"labels\"] = df_wav2vec[\"label\"]\n",
    "\n",
    "\n",
    "merged = []\n",
    "current = None\n",
    "\n",
    "for _, row in df_wav2vec.iterrows():\n",
    "    if current is None:\n",
    "        current = row.to_dict()\n",
    "        continue\n",
    "\n",
    "    if row[\"label\"] == current[\"label\"] and row[\"start\"] <= current[\"end\"] + 0.04:\n",
    "        # same sound, extend it\n",
    "        current[\"end\"] = row[\"end\"]\n",
    "    else:\n",
    "        merged.append(current)\n",
    "        current = row.to_dict()\n",
    "\n",
    "if current:\n",
    "    merged.append(current)\n",
    "\n",
    "df_wav2vec_merged = pd.DataFrame(merged)\n",
    "df_wav2vec_merged[\"duration\"] = (\n",
    "    df_wav2vec_merged[\"end\"] - df_wav2vec_merged[\"start\"]\n",
    ")\n",
    "\n",
    "pauses = []\n",
    "\n",
    "for i in range(1, len(events)):\n",
    "    gap = events[i][\"start\"] - events[i-1][\"end\"]\n",
    "    if gap >= 0.3:  # 300 ms\n",
    "        pauses.append({\n",
    "            \"type\": \"pause\",\n",
    "            \"start\": events[i-1][\"end\"],\n",
    "            \"end\": events[i][\"start\"],\n",
    "            \"duration\": gap\n",
    "        })\n",
    "\n",
    "pauses;\n",
    "\n",
    "\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "def load_audio(path):\n",
    "    audio, sr = sf.read(path, dtype=\"float32\")\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)  # stereo → mono\n",
    "    if sr != 16000:\n",
    "        audio = torchaudio.functional.resample(\n",
    "            torch.from_numpy(audio), sr, 16000\n",
    "        ).numpy()\n",
    "        sr = 16000\n",
    "    return audio, sr\n",
    "\n",
    "audio, sr = load_audio(audio_file_path)\n",
    "\n",
    "import whisperx\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "align_model, metadata = whisperx.load_align_model(\n",
    "    language_code=\"en\",  # or result[\"language\"] if available\n",
    "    device=device\n",
    ")\n",
    "\n",
    "aligned = whisperx.align(\n",
    "    result[\"segments\"],  # <-- your existing Whisper output\n",
    "    align_model,\n",
    "    metadata,\n",
    "    audio,\n",
    "    device\n",
    ")\n",
    "\n",
    "aligned_words = []\n",
    "\n",
    "for seg in aligned[\"segments\"]:\n",
    "    for w in seg.get(\"words\", []):\n",
    "        if w[\"start\"] is not None and w[\"end\"] is not None:\n",
    "            aligned_words.append({\n",
    "                \"word\": w[\"word\"].strip().lower(),\n",
    "                \"start\": float(w[\"start\"]),\n",
    "                \"end\": float(w[\"end\"])\n",
    "            })\n",
    "\n",
    "df_aligned_words = pd.DataFrame(aligned_words)\n",
    "df_aligned_words = df_aligned_words.sort_values(\"start\").reset_index(drop=True)\n",
    "# df_aligned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "896c512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>raw_label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>16.06</td>\n",
       "      <td>16.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>18.88</td>\n",
       "      <td>20.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.016718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>23.72</td>\n",
       "      <td>23.74</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>N</td>\n",
       "      <td>23.88</td>\n",
       "      <td>23.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>d</td>\n",
       "      <td>D</td>\n",
       "      <td>24.06</td>\n",
       "      <td>24.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>uh,</td>\n",
       "      <td>24.26</td>\n",
       "      <td>24.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.674564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.889380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>26.56</td>\n",
       "      <td>26.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>32.30</td>\n",
       "      <td>32.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>33.50</td>\n",
       "      <td>33.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>33.74</td>\n",
       "      <td>33.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>33.88</td>\n",
       "      <td>33.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>33.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>I</td>\n",
       "      <td>34.26</td>\n",
       "      <td>34.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>38.70</td>\n",
       "      <td>38.72</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>I</td>\n",
       "      <td>38.86</td>\n",
       "      <td>38.88</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>l</td>\n",
       "      <td>L</td>\n",
       "      <td>38.94</td>\n",
       "      <td>38.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>l</td>\n",
       "      <td>L</td>\n",
       "      <td>39.02</td>\n",
       "      <td>39.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>39.18</td>\n",
       "      <td>39.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>43.76</td>\n",
       "      <td>43.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>47.96</td>\n",
       "      <td>47.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>51.68</td>\n",
       "      <td>51.70</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>y</td>\n",
       "      <td>Y</td>\n",
       "      <td>51.78</td>\n",
       "      <td>51.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>54.88</td>\n",
       "      <td>54.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>63.26</td>\n",
       "      <td>63.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>64.74</td>\n",
       "      <td>64.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>66.62</td>\n",
       "      <td>66.66</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>r</td>\n",
       "      <td>R</td>\n",
       "      <td>68.14</td>\n",
       "      <td>68.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>r</td>\n",
       "      <td>R</td>\n",
       "      <td>69.32</td>\n",
       "      <td>69.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>w</td>\n",
       "      <td>W</td>\n",
       "      <td>69.62</td>\n",
       "      <td>69.64</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>69.80</td>\n",
       "      <td>69.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>d</td>\n",
       "      <td>D</td>\n",
       "      <td>69.90</td>\n",
       "      <td>69.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>70.08</td>\n",
       "      <td>70.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>73.30</td>\n",
       "      <td>73.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>81.38</td>\n",
       "      <td>81.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>t</td>\n",
       "      <td>T</td>\n",
       "      <td>84.74</td>\n",
       "      <td>84.78</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>85.94</td>\n",
       "      <td>85.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>g</td>\n",
       "      <td>G</td>\n",
       "      <td>86.38</td>\n",
       "      <td>86.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>s</td>\n",
       "      <td>S</td>\n",
       "      <td>93.50</td>\n",
       "      <td>93.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     style     type text raw_label  start    end  duration  confidence\n",
       "0   subtle   filler   uh         A  16.06  16.08      0.02         NaN\n",
       "1    clear   filler   um       um,  18.88  20.06      1.18    0.016718\n",
       "2   subtle   filler   uh         A  23.72  23.74      0.02         NaN\n",
       "3   subtle   filler   um         N  23.88  23.92      0.04         NaN\n",
       "4   subtle  stutter    d         D  24.06  24.08      0.02         NaN\n",
       "5    clear   filler   uh       uh,  24.26  24.38      0.12    0.674564\n",
       "6   subtle   filler   uh         O  24.88  24.90      0.02         NaN\n",
       "7    clear   filler   um       um,  26.00  26.22      0.22    0.889380\n",
       "8   subtle   filler   uh         U  26.56  26.60      0.04         NaN\n",
       "9   subtle   filler   uh         A  32.30  32.32      0.02         NaN\n",
       "10  subtle   filler   uh         A  33.50  33.52      0.02         NaN\n",
       "11  subtle  stutter    b         B  33.74  33.76      0.02         NaN\n",
       "12  subtle  stutter    t         T  33.88  33.90      0.02         NaN\n",
       "13  subtle  stutter    s         S  33.98  34.00      0.02         NaN\n",
       "14  subtle   filler   uh         I  34.26  34.28      0.02         NaN\n",
       "15  subtle  stutter    s         S  38.70  38.72      0.02         NaN\n",
       "16  subtle   filler   uh         I  38.86  38.88      0.02         NaN\n",
       "17  subtle  stutter    l         L  38.94  38.96      0.02         NaN\n",
       "18  subtle  stutter    l         L  39.02  39.04      0.02         NaN\n",
       "19  subtle  stutter    t         T  39.18  39.20      0.02         NaN\n",
       "20  subtle  stutter    s         S  43.76  43.78      0.02         NaN\n",
       "21  subtle   filler   uh         A  47.96  47.98      0.02         NaN\n",
       "22  subtle  stutter    t         T  51.68  51.70      0.02         NaN\n",
       "23  subtle  stutter    y         Y  51.78  51.80      0.02         NaN\n",
       "24  subtle  stutter    t         T  54.88  54.90      0.02         NaN\n",
       "25  subtle   filler   uh         U  63.26  63.28      0.02         NaN\n",
       "26  subtle   filler   uh         O  64.74  64.76      0.02         NaN\n",
       "27  subtle  stutter    b         B  66.62  66.66      0.04         NaN\n",
       "28  subtle  stutter    r         R  68.14  68.16      0.02         NaN\n",
       "29  subtle  stutter    r         R  69.32  69.34      0.02         NaN\n",
       "30  subtle  stutter    w         W  69.62  69.64      0.02         NaN\n",
       "31  subtle  stutter    t         T  69.80  69.82      0.02         NaN\n",
       "32  subtle  stutter    d         D  69.90  69.92      0.02         NaN\n",
       "33  subtle  stutter    t         T  70.08  70.12      0.04         NaN\n",
       "34  subtle  stutter    t         T  73.30  73.32      0.02         NaN\n",
       "35  subtle   filler   uh         A  81.38  81.40      0.02         NaN\n",
       "36  subtle  stutter    t         T  84.74  84.78      0.04         NaN\n",
       "37  subtle   filler   uh         U  85.94  85.96      0.02         NaN\n",
       "38  subtle  stutter    g         G  86.38  86.40      0.02         NaN\n",
       "39  subtle  stutter    s         S  93.50  93.52      0.02         NaN"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import whisper\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "\n",
    "FILLER_MAP = {\n",
    "    \"A\": \"uh\",\n",
    "    \"E\": \"uh\",\n",
    "    \"U\": \"uh\",\n",
    "    \"M\": \"um\",\n",
    "    \"N\": \"um\",\n",
    "    \"MM\": \"um\",\n",
    "    \"NN\": \"um\",\n",
    "}\n",
    "\n",
    "WORD_ONSET_WINDOW = 0.12       # window BEFORE word start\n",
    "MIN_FILLER_DURATION = 0.02\n",
    "STUTTER_CONSONANTS = set(\"BCDFGHJKLPQRSTVWXYZ\")\n",
    "\n",
    "# ==============================\n",
    "# HELPERS\n",
    "# ==============================\n",
    "\n",
    "def overlaps_any_word_relaxed(start, end, words, tol_before=0.02, tol_after=0.02):\n",
    "    for _, w in words.iterrows():\n",
    "        if start < w[\"end\"] + tol_after and end > w[\"start\"] - tol_before:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_word_initial_candidate(row, word_starts, max_lead=WORD_ONSET_WINDOW):\n",
    "    end = row[\"end\"]\n",
    "    upcoming = word_starts[\n",
    "        (word_starts[\"start\"] >= end) &\n",
    "        ((word_starts[\"start\"] - end) <= max_lead)\n",
    "    ]\n",
    "    return not upcoming.empty\n",
    "\n",
    "\n",
    "def merge_adjacent_events(df, max_gap=0.05):\n",
    "    merged = []\n",
    "    current = None\n",
    "\n",
    "    for _, row in df.sort_values(\"start\").iterrows():\n",
    "        if current is None:\n",
    "            current = row.to_dict()\n",
    "            continue\n",
    "\n",
    "        same_label = row[\"labels\"] == current[\"labels\"]\n",
    "        close = row[\"start\"] - current[\"end\"] <= max_gap\n",
    "\n",
    "        if same_label and close:\n",
    "            current[\"end\"] = row[\"end\"]\n",
    "            current[\"duration\"] += row[\"duration\"]\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            current = row.to_dict()\n",
    "\n",
    "    if current:\n",
    "        merged.append(current)\n",
    "\n",
    "    return pd.DataFrame(merged)\n",
    "\n",
    "\n",
    "def looks_like_filler(norm, duration):\n",
    "    if duration < MIN_FILLER_DURATION:\n",
    "        return False\n",
    "\n",
    "    # vowel hesitations (uh, ah, eh, h)\n",
    "    if re.fullmatch(r\"[AEIOUH]+\", norm):\n",
    "        return True\n",
    "\n",
    "    # nasal hums (mm, nn)\n",
    "    if re.fullmatch(r\"M+|N+\", norm):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def should_suppress_word_initial(row):\n",
    "    label = row[\"labels\"].upper()\n",
    "    norm = re.sub(r\"(.)\\1+\", r\"\\1\", label)\n",
    "\n",
    "    # NEVER suppress filler-shaped sounds\n",
    "    if looks_like_filler(norm, row[\"duration\"]):\n",
    "        return False\n",
    "\n",
    "    # suppress only ultra-short junk\n",
    "    return row[\"duration\"] < 0.03\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: REMOVE WORD-OVERLAPS\n",
    "# ==============================\n",
    "\n",
    "df_wav2vec_merged = df_wav2vec_merged.copy()\n",
    "\n",
    "df_wav2vec_merged[\"overlaps_word\"] = df_wav2vec_merged.apply(\n",
    "    lambda r: overlaps_any_word_relaxed(\n",
    "        r[\"start\"], r[\"end\"], df_aligned_words\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word = df_wav2vec_merged.loc[\n",
    "    ~df_wav2vec_merged[\"overlaps_word\"]\n",
    "].copy()\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: WORD-START HANDLING (FIXED)\n",
    "# ==============================\n",
    "\n",
    "word_starts = df_aligned_words[[\"start\"]].copy()\n",
    "\n",
    "df_non_word[\"is_word_initial\"] = df_non_word.apply(\n",
    "    lambda r: is_word_initial_candidate(r, word_starts),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word[\"suppress\"] = df_non_word.apply(\n",
    "    lambda r: r[\"is_word_initial\"] and should_suppress_word_initial(r),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_non_word = df_non_word.loc[\n",
    "    ~df_non_word[\"suppress\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: MERGE MICRO EVENTS\n",
    "# ==============================\n",
    "\n",
    "df_non_word = merge_adjacent_events(df_non_word, max_gap=0.05)\n",
    "\n",
    "# ==============================\n",
    "# STEP 4: CLASSIFY WAV2VEC EVENTS\n",
    "# ==============================\n",
    "\n",
    "def classify_non_word_event(row):\n",
    "    label = row[\"labels\"].upper()\n",
    "    duration = row[\"duration\"]\n",
    "    norm = re.sub(r\"(.)\\1+\", r\"\\1\", label)\n",
    "\n",
    "    # ---- FILLERS ----\n",
    "    if looks_like_filler(norm, duration):\n",
    "        return {\n",
    "            \"type\": \"filler\",\n",
    "            \"raw_label\": label,\n",
    "            \"text\": FILLER_MAP.get(norm, \"uh\"),\n",
    "        }\n",
    "\n",
    "    # ---- STUTTERS ----\n",
    "    if (\n",
    "        norm in STUTTER_CONSONANTS\n",
    "        and norm not in FILLER_MAP\n",
    "        and duration < 0.15\n",
    "    ):\n",
    "        return {\n",
    "            \"type\": \"stutter\",\n",
    "            \"raw_label\": label,\n",
    "            \"text\": norm.lower(),\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "converted = []\n",
    "\n",
    "for _, row in df_non_word.iterrows():\n",
    "    result = classify_non_word_event(row)\n",
    "    if result:\n",
    "        converted.append({\n",
    "            \"type\": result[\"type\"],\n",
    "            \"text\": result[\"text\"],\n",
    "            \"raw_label\": result[\"raw_label\"],\n",
    "            \"start\": row[\"start\"],\n",
    "            \"end\": row[\"end\"],\n",
    "            \"duration\": row[\"duration\"],\n",
    "        })\n",
    "\n",
    "df_filler_events = pd.DataFrame(converted)\n",
    "\n",
    "# ==============================\n",
    "# STEP 5: WHISPER VERBATIM FILLERS\n",
    "# ==============================\n",
    "\n",
    "filler_model = whisper.load_model(\"base\", device=\"cpu\")\n",
    "\n",
    "verbatim_result = filler_model.transcribe(\n",
    "    audio_file_path,\n",
    "    task=\"transcribe\",\n",
    "    temperature=0,\n",
    "    word_timestamps=True,\n",
    "    condition_on_previous_text=False,\n",
    "    initial_prompt=(\n",
    "        \"Transcribe verbatim. Include filler words like um, uh, er, \"\n",
    "        \"false starts, repetitions, and hesitations.\"\n",
    "    ),\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "FILLER_PATTERN = re.compile(\n",
    "    r\"^(um+|uh+|erm+|er+|ah+|eh+)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def normalize_whisper_token(token):\n",
    "    token = token.lower().strip()\n",
    "    return re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", token)\n",
    "\n",
    "\n",
    "whisper_fillers = []\n",
    "\n",
    "for seg in verbatim_result.get(\"segments\", []):\n",
    "    for w in seg.get(\"words\", []):\n",
    "        norm = normalize_whisper_token(w[\"word\"])\n",
    "        if norm and FILLER_PATTERN.match(norm):\n",
    "            whisper_fillers.append({\n",
    "                \"style\": \"clear\",\n",
    "                \"type\": \"filler\",\n",
    "                \"text\": norm,\n",
    "                \"raw_label\": w[\"word\"],\n",
    "                \"start\": float(w[\"start\"]),\n",
    "                \"end\": float(w[\"end\"]),\n",
    "                \"duration\": float(w[\"end\"] - w[\"start\"]),\n",
    "                \"confidence\": float(w[\"probability\"]),\n",
    "            })\n",
    "\n",
    "df_whisper_fillers = pd.DataFrame(whisper_fillers)\n",
    "\n",
    "# ==============================\n",
    "# STEP 6: MERGE (WHISPER FIRST, WAV2VEC BACKFILL)\n",
    "# ==============================\n",
    "\n",
    "def overlaps_time(a_start, a_end, b_start, b_end, tol=0.05):\n",
    "    return (a_start < b_end + tol) and (a_end > b_start - tol)\n",
    "\n",
    "\n",
    "final_fillers = []\n",
    "\n",
    "for _, wf in df_whisper_fillers.iterrows():\n",
    "    final_fillers.append(wf.to_dict())\n",
    "\n",
    "for _, row in df_filler_events.iterrows():\n",
    "    duplicate = False\n",
    "\n",
    "    for af in final_fillers:\n",
    "        if overlaps_time(\n",
    "            row[\"start\"], row[\"end\"],\n",
    "            af[\"start\"], af[\"end\"]\n",
    "        ):\n",
    "            duplicate = True\n",
    "            break\n",
    "\n",
    "    if not duplicate:\n",
    "        final_fillers.append({\n",
    "            \"style\": \"subtle\",\n",
    "            \"type\": row[\"type\"],\n",
    "            \"text\": row[\"text\"],\n",
    "            \"raw_label\": row[\"raw_label\"],\n",
    "            \"start\": row[\"start\"],\n",
    "            \"end\": row[\"end\"],\n",
    "            \"duration\": row[\"duration\"],\n",
    "        })\n",
    "\n",
    "df_final_fillers = pd.DataFrame(final_fillers)\n",
    "\n",
    "if not df_final_fillers.empty:\n",
    "    df_final_fillers = (\n",
    "        df_final_fillers\n",
    "        .sort_values(\"start\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "df_final_fillers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c5167103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>raw_label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>confidence</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>16.06</td>\n",
       "      <td>16.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>18.88</td>\n",
       "      <td>20.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>23.72</td>\n",
       "      <td>23.74</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>N</td>\n",
       "      <td>23.88</td>\n",
       "      <td>23.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>uh,</td>\n",
       "      <td>24.26</td>\n",
       "      <td>24.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>24.88</td>\n",
       "      <td>24.90</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clear</td>\n",
       "      <td>filler</td>\n",
       "      <td>um</td>\n",
       "      <td>um,</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.889380</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>26.56</td>\n",
       "      <td>26.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>32.30</td>\n",
       "      <td>32.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>33.50</td>\n",
       "      <td>33.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>I</td>\n",
       "      <td>34.26</td>\n",
       "      <td>34.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>I</td>\n",
       "      <td>38.86</td>\n",
       "      <td>38.88</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>subtle</td>\n",
       "      <td>stutter</td>\n",
       "      <td>l</td>\n",
       "      <td>L</td>\n",
       "      <td>38.94</td>\n",
       "      <td>39.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>47.96</td>\n",
       "      <td>47.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>63.26</td>\n",
       "      <td>63.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>O</td>\n",
       "      <td>64.74</td>\n",
       "      <td>64.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>A</td>\n",
       "      <td>81.38</td>\n",
       "      <td>81.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>subtle</td>\n",
       "      <td>filler</td>\n",
       "      <td>uh</td>\n",
       "      <td>U</td>\n",
       "      <td>85.94</td>\n",
       "      <td>85.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     style     type text raw_label  start    end  duration  confidence  count\n",
       "0   subtle   filler   uh         A  16.06  16.08      0.02         NaN    NaN\n",
       "1    clear   filler   um       um,  18.88  20.06      1.18    0.016718    NaN\n",
       "2   subtle   filler   uh         A  23.72  23.74      0.02         NaN    NaN\n",
       "3   subtle   filler   um         N  23.88  23.92      0.04         NaN    NaN\n",
       "4    clear   filler   uh       uh,  24.26  24.38      0.12    0.674564    NaN\n",
       "5   subtle   filler   uh         O  24.88  24.90      0.02         NaN    NaN\n",
       "6    clear   filler   um       um,  26.00  26.22      0.22    0.889380    NaN\n",
       "7   subtle   filler   uh         U  26.56  26.60      0.04         NaN    NaN\n",
       "8   subtle   filler   uh         A  32.30  32.32      0.02         NaN    NaN\n",
       "9   subtle   filler   uh         A  33.50  33.52      0.02         NaN    NaN\n",
       "10  subtle   filler   uh         I  34.26  34.28      0.02         NaN    NaN\n",
       "11  subtle   filler   uh         I  38.86  38.88      0.02         NaN    NaN\n",
       "12  subtle  stutter    l         L  38.94  39.04      0.10         NaN    2.0\n",
       "13  subtle   filler   uh         A  47.96  47.98      0.02         NaN    NaN\n",
       "14  subtle   filler   uh         U  63.26  63.28      0.02         NaN    NaN\n",
       "15  subtle   filler   uh         O  64.74  64.76      0.02         NaN    NaN\n",
       "16  subtle   filler   uh         A  81.38  81.40      0.02         NaN    NaN\n",
       "17  subtle   filler   uh         U  85.94  85.96      0.02         NaN    NaN"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stutters = df_final_fillers[\n",
    "    df_final_fillers[\"type\"] == \"stutter\"\n",
    "].copy()\n",
    "\n",
    "df_fillers = df_final_fillers[\n",
    "    df_final_fillers[\"type\"] == \"filler\"\n",
    "].copy()\n",
    "\n",
    "GROUP_GAP_SEC = 0.15  # max gap between repetitions\n",
    "\n",
    "# ==============================\n",
    "# GROUP STUTTERS\n",
    "# ==============================\n",
    "\n",
    "grouped = []\n",
    "current = None\n",
    "\n",
    "for _, row in df_stutters.sort_values(\"start\").iterrows():\n",
    "    if current is None:\n",
    "        current = row.to_dict()\n",
    "        current[\"count\"] = 1\n",
    "        continue\n",
    "\n",
    "    same_sound = row[\"raw_label\"] == current[\"raw_label\"]\n",
    "    close_in_time = row[\"start\"] - current[\"end\"] <= GROUP_GAP_SEC\n",
    "\n",
    "    if same_sound and close_in_time:\n",
    "        current[\"end\"] = row[\"end\"]\n",
    "        current[\"duration\"] = current[\"end\"] - current[\"start\"]\n",
    "        current[\"count\"] += 1\n",
    "    else:\n",
    "        grouped.append(current)\n",
    "        current = row.to_dict()\n",
    "        current[\"count\"] = 1\n",
    "\n",
    "if current:\n",
    "    grouped.append(current)\n",
    "\n",
    "df_grouped_stutters = pd.DataFrame(grouped)\n",
    "\n",
    "\n",
    "df_final_clean = ( \n",
    "    pd.concat([df_fillers, df_grouped_stutters], ignore_index=True) \n",
    "    .sort_values(\"start\") \n",
    "    .reset_index(drop=True) \n",
    ")\n",
    "\n",
    "# --- HARD STUTTER FILTER --- \n",
    "df_final_clean = df_final_clean[\n",
    "    ~(\n",
    "        (df_final_clean[\"type\"] == \"stutter\") & \n",
    "        (df_final_clean[\"count\"].fillna(1) < 2) & \n",
    "        (df_final_clean[\"duration\"] < 0.15) \n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_final_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cce14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wpm': 117.48341536486198,\n",
       " 'fillers_per_min': 10.913759897282262,\n",
       " 'stutters_per_min': 14.765675155146589,\n",
       " 'long_pauses_per_min': 3.2099293815536063,\n",
       " 'very_long_pauses_per_min': 0.6419858763107212,\n",
       " 'pause_time_ratio': np.float64(0.13931093515942625),\n",
       " 'pause_variability': np.float64(0.284486902987219),\n",
       " 'vocab_richness': 0.5355191256830601,\n",
       " 'repetition_ratio': np.float64(0.08196721311475409)}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def filler_weight(duration):\n",
    "    \"\"\"\n",
    "    Weight fillers by perceptual impact.\n",
    "    \"\"\"\n",
    "    if duration < 0.08:\n",
    "        return 0.2      # micro hesitation\n",
    "    elif duration < 0.3:\n",
    "        return 0.6      # subtle filler\n",
    "    else:\n",
    "        return 1.0      # real filler\n",
    "    \n",
    "def overlaps_filler(start, end, fillers, tol=0.05):\n",
    "    for _, f in fillers.iterrows():\n",
    "        if start < f[\"end\"] + tol and end > f[\"start\"] - tol:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "# ---------- ENHANCED NORMALIZATION ----------\n",
    "\n",
    "duration_min = max(total_duration / 60.0, 0.5)\n",
    "\n",
    "# Fillers & stutters\n",
    "# Use cleaned events only (grouped + filtered)\n",
    "filler_events = df_final_clean[df_final_clean[\"type\"] == \"filler\"]\n",
    "stutter_events = df_final_clean[df_final_clean[\"type\"] == \"stutter\"]\n",
    "\n",
    "\n",
    "fillers_per_min = (\n",
    "    filler_events[\"duration\"]\n",
    "    .apply(filler_weight)\n",
    "    .sum()\n",
    "    / duration_min\n",
    ")\n",
    "stutters_per_min = len(stutter_events) / duration_min\n",
    "\n",
    "# Pauses\n",
    "clean_pauses = []\n",
    "\n",
    "for i in range(1, len(df_words)):\n",
    "    gap_start = df_words.iloc[i-1][\"end\"]\n",
    "    gap_end = df_words.iloc[i][\"start\"]\n",
    "    gap = gap_end - gap_start\n",
    "\n",
    "    if gap > 0.3 and not overlaps_filler(\n",
    "        gap_start, gap_end, df_final_clean\n",
    "    ):\n",
    "        clean_pauses.append(gap)\n",
    "\n",
    "clean_pauses = pd.Series(clean_pauses)\n",
    "\n",
    "long_pauses = clean_pauses[clean_pauses > 1.0]\n",
    "very_long_pauses = clean_pauses[clean_pauses > 2.0]\n",
    "\n",
    "long_pauses_per_min = len(long_pauses) / duration_min\n",
    "very_long_pauses_per_min = len(very_long_pauses) / duration_min\n",
    "pause_time_ratio = clean_pauses.sum() / total_duration if len(clean_pauses) else 0.0\n",
    "pause_variability = clean_pauses.std() if len(clean_pauses) > 5 else 0.0\n",
    "\n",
    "\n",
    "long_pauses_per_min = len(long_pauses) / duration_min\n",
    "very_long_pauses_per_min = len(very_long_pauses) / duration_min\n",
    "\n",
    "# Lexical\n",
    "words_clean = df_words[\"word\"].str.lower()\n",
    "vocab_richness = words_clean.nunique() / len(words_clean)\n",
    "repetition_ratio = words_clean.value_counts().iloc[0] / len(words_clean)\n",
    "\n",
    "# Temporal fluency instability (variance of gaps)\n",
    "pause_variability = pauses.std() if len(pauses) > 5 else 0.0\n",
    "\n",
    "normalized_metrics = {\n",
    "    \"wpm\": words_per_minute,\n",
    "    \"fillers_per_min\": fillers_per_min,\n",
    "    \"stutters_per_min\": stutters_per_min,\n",
    "    \"long_pauses_per_min\": long_pauses_per_min,\n",
    "    \"very_long_pauses_per_min\": very_long_pauses_per_min,\n",
    "    \"pause_time_ratio\": pause_time_ratio,\n",
    "    \"pause_variability\": pause_variability,\n",
    "    \"vocab_richness\": vocab_richness,\n",
    "    \"repetition_ratio\": repetition_ratio,\n",
    "}\n",
    "\n",
    "normalized_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee627c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'issue': 'filler_dependency',\n",
       "  'severity': 'high',\n",
       "  'root_cause': 'Fillers replace silent planning pauses.',\n",
       "  'score_impact': 25},\n",
       " {'issue': 'hesitation_structure',\n",
       "  'severity': 'high',\n",
       "  'root_cause': 'Pauses frequently interrupt sentence flow.',\n",
       "  'score_impact': 24},\n",
       " {'issue': 'delivery_instability',\n",
       "  'severity': 'medium',\n",
       "  'root_cause': 'Speech rhythm varies unpredictably.',\n",
       "  'score_impact': 8}]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_CONFIG = {\n",
    "    \"conversational\": {\n",
    "        \"pause_tolerance\": 1.0,\n",
    "        \"pause_variability_tolerance\": 1.0,\n",
    "    },\n",
    "    \"narrative\": {\n",
    "        \"pause_tolerance\": 1.4,          # allow more long pauses\n",
    "        \"pause_variability_tolerance\": 1.3,\n",
    "    },\n",
    "    \"presentation\": {\n",
    "        \"pause_tolerance\": 1.2,\n",
    "        \"pause_variability_tolerance\": 1.1,\n",
    "    },\n",
    "    \"interview\": {\n",
    "        \"pause_tolerance\": 0.9,          # stricter\n",
    "        \"pause_variability_tolerance\": 0.9,\n",
    "    },\n",
    "}\n",
    "\n",
    "context = CONTEXT_CONFIG.get(speech_context, CONTEXT_CONFIG[\"conversational\"])\n",
    "\n",
    "# %%\n",
    "# ---------- CONSISTENT FLUENCY SCORING ----------\n",
    "\n",
    "import math\n",
    "\n",
    "def clamp01(x):\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "\n",
    "# ---- Subscores (0–1, higher = better) ----\n",
    "\n",
    "# Speech rate (ASYMMETRIC)\n",
    "wpm = normalized_metrics[\"wpm\"]\n",
    "\n",
    "if wpm < 110:  # too slow hurts fluency hard\n",
    "    speech_rate_score = clamp01((wpm - 70) / 40)\n",
    "elif wpm <= 170:  # optimal band\n",
    "    speech_rate_score = 1.0\n",
    "else:  # too fast hurts gently\n",
    "    speech_rate_score = clamp01(1 - (wpm - 170) / 120)\n",
    "\n",
    "\n",
    "# Pauses (structural)\n",
    "pause_score = clamp01(\n",
    "    1 - (\n",
    "        normalized_metrics[\"long_pauses_per_min\"]\n",
    "        / (4.0 * context[\"pause_tolerance\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fillers (structural)\n",
    "filler_score = clamp01(\n",
    "    1 - (normalized_metrics[\"fillers_per_min\"] / 6.0)\n",
    ")\n",
    "\n",
    "# Stability (structural)\n",
    "stability_score = clamp01(\n",
    "    1 - (\n",
    "        normalized_metrics[\"pause_variability\"]\n",
    "        / (0.7 * context[\"pause_variability_tolerance\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Lexical quality (STYLE)\n",
    "lexical_score = clamp01(\n",
    "    0.65 * normalized_metrics[\"vocab_richness\"]\n",
    "    + 0.35 * (1 - normalized_metrics[\"repetition_ratio\"])\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Weighted aggregation ----\n",
    "# Structural dimensions dominate readiness\n",
    "raw_score = (\n",
    "    0.30 * pause_score +\n",
    "    0.25 * filler_score +\n",
    "    0.20 * stability_score +\n",
    "    0.15 * speech_rate_score +\n",
    "    0.10 * lexical_score\n",
    ")\n",
    "\n",
    "fluency_score = int(round(100 * clamp01(raw_score)))\n",
    "fluency_score\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# ---------- ISSUE DETECTION ----------\n",
    "\n",
    "issues = []\n",
    "\n",
    "def issue(severity, issue_id, root_cause, score_impact):\n",
    "    return {\n",
    "        \"issue\": issue_id,\n",
    "        \"severity\": severity,\n",
    "        \"root_cause\": root_cause,\n",
    "        \"score_impact\": score_impact\n",
    "    }\n",
    "\n",
    "\n",
    "# Structural blockers\n",
    "if pause_score < 0.6:\n",
    "    issues.append(issue(\n",
    "        \"high\",\n",
    "        \"hesitation_structure\",\n",
    "        \"Pauses frequently interrupt sentence flow.\",\n",
    "        int((1 - pause_score) * 30)\n",
    "    ))\n",
    "\n",
    "if filler_score < 0.6:\n",
    "    issues.append(issue(\n",
    "        \"high\",\n",
    "        \"filler_dependency\",\n",
    "        \"Fillers replace silent planning pauses.\",\n",
    "        int((1 - filler_score) * 25)\n",
    "    ))\n",
    "\n",
    "if stability_score < 0.6:\n",
    "    issues.append(issue(\n",
    "        \"medium\",\n",
    "        \"delivery_instability\",\n",
    "        \"Speech rhythm varies unpredictably.\",\n",
    "        int((1 - stability_score) * 20)\n",
    "    ))\n",
    "\n",
    "# Style issues (never blockers alone)\n",
    "if speech_rate_score < 0.7:\n",
    "    issues.append(issue(\n",
    "        \"medium\",\n",
    "        \"delivery_pacing\",\n",
    "        \"Speech rate is faster than optimal for clarity.\",\n",
    "        int((1 - speech_rate_score) * 15)\n",
    "    ))\n",
    "\n",
    "if lexical_score < 0.5:\n",
    "    issues.append(issue(\n",
    "        \"low\",\n",
    "        \"lexical_simplicity\",\n",
    "        \"Frequent reuse of common vocabulary.\",\n",
    "        int((1 - lexical_score) * 10)\n",
    "    ))\n",
    "\n",
    "\n",
    "issues = sorted(issues, key=lambda x: x[\"score_impact\"], reverse=True)\n",
    "issues\n",
    "\n",
    "\n",
    "# %%\n",
    "# ---------- READINESS JUDGMENT ----------\n",
    "if total_duration < 30:\n",
    "    readiness = \"insufficient_sample\"\n",
    "\n",
    "high_issues = [i for i in issues if i[\"severity\"] == \"high\"]\n",
    "medium_issues = [i for i in issues if i[\"severity\"] == \"medium\"]\n",
    "\n",
    "if len(high_issues) >= 2:\n",
    "    readiness = \"not_ready\"\n",
    "elif len(high_issues) == 1:\n",
    "    readiness = \"borderline\"\n",
    "elif len(medium_issues) >= 2:\n",
    "    readiness = \"borderline\"\n",
    "elif fluency_score >= 80:\n",
    "    readiness = \"ready\"\n",
    "else:\n",
    "    readiness = \"borderline\"\n",
    "\n",
    "readiness\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# ---------- BENCHMARKING ----------\n",
    "\n",
    "# v1 calibrated bands (replace with data later)\n",
    "if fluency_score >= 85:\n",
    "    percentile = 80\n",
    "elif fluency_score >= 75:\n",
    "    percentile = 65\n",
    "elif fluency_score >= 65:\n",
    "    percentile = 50\n",
    "else:\n",
    "    percentile = 30\n",
    "\n",
    "score_gap = (\n",
    "    max(0, 80 - fluency_score)\n",
    "    if readiness != \"ready\"\n",
    "    else 0\n",
    ")\n",
    "\n",
    "benchmarking = {\n",
    "    \"percentile\": percentile,\n",
    "    \"target_score\": 80,\n",
    "    \"score_gap\": score_gap,\n",
    "    \"estimated_guided_practice_hours\": score_gap * 0.6,\n",
    "}\n",
    "\n",
    "\n",
    "benchmarking\n",
    "\n",
    "\n",
    "# %%\n",
    "# ---------- OPINIONS ----------\n",
    "\n",
    "action_plan = []\n",
    "\n",
    "max_gain = sum(i[\"score_impact\"] for i in issues[:3]) or 1\n",
    "scale = score_gap / max_gain if score_gap > 0 else 1.0\n",
    "\n",
    "for idx, issue in enumerate(issues[:3]):\n",
    "    action_plan.append({\n",
    "        \"priority\": idx + 1,\n",
    "        \"focus\": issue[\"issue\"],\n",
    "        \"instruction\": {\n",
    "            \"hesitation_structure\": \"Pause only after completing full clauses.\",\n",
    "            \"filler_dependency\": \"Replace fillers with silent pauses under 300ms.\",\n",
    "            \"delivery_instability\": \"Practice steady pacing with metronome drills.\",\n",
    "            \"delivery_pacing\": \"Reduce speed slightly while maintaining energy.\",\n",
    "            \"lexical_simplicity\": \"Actively substitute repeated words during rehearsal.\"\n",
    "        }[issue[\"issue\"]],\n",
    "        \"expected_score_gain\": int(issue[\"score_impact\"] * scale)\n",
    "    })\n",
    "\n",
    "opinions = {\n",
    "    \"primary_issues\": issues,\n",
    "    \"action_plan\": action_plan\n",
    "}\n",
    "\n",
    "opinions\n",
    "\n",
    "\n",
    "# %%\n",
    "final_response = {\n",
    "    \"verdict\": {\n",
    "        \"fluency_score\": fluency_score,\n",
    "        \"readiness\": readiness,\n",
    "    },\n",
    "\n",
    "    \"benchmarking\": benchmarking,\n",
    "\n",
    "    \"normalized_metrics\": normalized_metrics,\n",
    "\n",
    "    \"opinions\": opinions,\n",
    "\n",
    "    \"word_timestamps\": df_words.to_dict(orient=\"records\"),\n",
    "\n",
    "    \"segment_timestamps\": df_segments.to_dict(orient=\"records\"),\n",
    "\n",
    "    \"filler_events\": df_final_fillers.to_dict(orient=\"records\"),\n",
    "\n",
    "    \"aligned_words\": df_aligned_words.to_dict(orient=\"records\"),\n",
    "}\n",
    "\n",
    "final_response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
